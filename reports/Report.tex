\documentclass[a4paper]{article}

\usepackage[sort]{natbib}
\usepackage{fancyhdr}


% \documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{booktabs} % To thicken table lines
\usepackage{tablefootnote}
\usepackage{listings}
% \usepackage[numbers]{natbib}

\usepackage{graphicx}
\usepackage{babel,blindtext}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}


% Uirá packages
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{booktabs} % To thicken table lines
\usepackage{graphicx}
\usepackage{babel,blindtext}
\usepackage{amsmath}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{epstopdf}



% you may include other packages here (next line)
\usepackage{enumitem}



%----- you must not change this -----------------
\oddsidemargin 0.2cm
\topmargin -1.0cm
\textheight 24.0cm
\textwidth 15.25cm
% \parindent=0pt
\parskip 1ex
\renewcommand{\baselinestretch}{1.1}
\pagestyle{fancy}
%----------------------------------------------------



% enter your details here----------------------------------

\lhead{\normalsize \textrm{Navigation}}
\chead{}
\rhead{\normalsize August 23, 2018}
\lfoot{\normalsize \textrm{DRLND - Udacity}}
\cfoot{}
\rfoot{Uirá Caiado}
\setlength{\fboxrule}{4pt}\setlength{\fboxsep}{2ex}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}


\begin{document}


%----------------your title below -----------------------------

\begin{center}

{\bf \large {Navigation Using Deep Reinforcement Learning \\ \small Uirá Caiado}}
\end{center}


%---------------- start of document body------------------

% The write up is approximately 1 page (500 words) and includes a summary of the paper (including new techniques introduced), and the key results (if any) that were achieved.

Bla vla. In their paper.

\cite{SchaulQAS15} and \cite{mnih2015humanlevel} and \cite{HasseltGS15}

As explained in the Deepmind's blog post\footnote{Source: \url{https://deepmind.com/research/alphago/}} about the computer program, due to the enormous search space of Go, traditional AI methods, which construct a search tree over all possible positions, are not suited to this board game. In general, we can reduce the search space in games using two principles: position evaluation and sampling actions from a policy. However, even though the application of both principles has provided superhuman performance in games like backgammon and Scrabble, as explained by Bla, they only produced a weak amateur level play in Go.


\begin{figure}[ht]
\centering
\captionsetup{margin=2cm}
\includegraphics[width=0.7\textwidth]{../notebooks/figures/reward_comp.eps}
\caption{Média móvel de $10$ períodos do P\&L do final do episódio, usando diferentes funções de \textit{reward}.}
\label{fig:rwd_funcs}
\end{figure}


Similarly to the approach described above, AlphaGo uses two deep convolutional neural networks in the place of the general principles. One neural network, the "value network", predicts how likely a move can lead to a win after a sequence of optimal moves, helping the program reducing the depth of the search. The other neural network, the "policy network", helps reduce the breadth of the search selecting the next move to play. Both neural networks are then combined with an advanced tree search called Monte Carlo Tree Search (MCTS) that effectively selects actions by look-ahead search.

This paper has introduced many different approaches to train the model. First, the policy network was trained directly from expert human moves, using supervised learning. This first stage helped AlphaGo develops its understanding of what reasonable human play looks like. Next, the policy network already trained by supervised learning was improved using reinforcement learning. The system played against different versions of itself, learning from its mistakes. Finally, the value network, which evaluates the value of a position of the program on the board, also was created using reinforcement learning using self-play data set.

After combining the policy and value network with MCTS, the paper reports that the program achieved 99.8\% winning rate against other Go programs, and defeated the human European Go champion by five games to 0. AlphaGo is based on deep neural networks that are trained by a combination of supervised and reinforcement learning, avoiding the construction of handcrafted evaluation function, as was did for DeepBlue, for example. The search algorithm developed to the program could be applied to other domains, as general game-playing, scheduling, and constraint satisfaction.



% ----------------end of document body---------------------

%---------------- start of references------------------

\bibliographystyle{plain}
% or try abbrvnat or unsrtnat
\bibliography{biblio.bib}

%---------------- end of references------------------


\end{document}
